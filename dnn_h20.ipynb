{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zia207/r-colab/blob/main/dnn_h20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1KiRoJmCj-f"
      },
      "source": [
        "# Deep Neural Network with h20\n",
        "\n",
        "Zia Ahmed, University at Buffalo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7D2F2aACzIA"
      },
      "source": [
        "\n",
        "H2O’s Deep Learning is based on a multi-layer feedforward artificial neural network that is trained with stochastic gradient descent using back-propagation. The network can contain a large number of hidden layers consisting of neurons with tanh, rectifier, and maxout activation functions. Advanced features such as adaptive learning rate, rate annealing, momentum training, dropout, L1 or L2 regularization, checkpointing, and grid search enable high predictive accuracy. Each compute node trains a copy of the global model parameters on its local data with multi-threading (asynchronously) and contributes periodically to the global model via model averaging across the network.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPIUmAb0Days"
      },
      "source": [
        "### Install h20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7kCbqijDa8X",
        "outputId": "5c81f8de-8b14-47d0-b340-3e12ece8ea8a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘gargle’, ‘googledrive’, ‘timechange’, ‘systemfonts’, ‘textshaping’, ‘vroom’, ‘dbplyr’, ‘dtplyr’, ‘forcats’, ‘googlesheets4’, ‘haven’, ‘hms’, ‘httr’, ‘jsonlite’, ‘lubridate’, ‘magrittr’, ‘modelr’, ‘ragg’, ‘readr’, ‘readxl’, ‘reprex’, ‘rvest’\n",
            "\n",
            "\n",
            "Warning message in install.packages(\"tidyverse\"):\n",
            "“installation of package ‘textshaping’ had non-zero exit status”\n",
            "Warning message in install.packages(\"tidyverse\"):\n",
            "“installation of package ‘ragg’ had non-zero exit status”\n",
            "Warning message in install.packages(\"tidyverse\"):\n",
            "“installation of package ‘readr’ had non-zero exit status”\n",
            "Warning message in install.packages(\"tidyverse\"):\n",
            "“installation of package ‘tidyverse’ had non-zero exit status”\n"
          ]
        }
      ],
      "source": [
        "install.packages('h2o')\n",
        "install.packages('tidymodels')\n",
        "install.packages('tidyverse')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKb6sLDeC7n7"
      },
      "source": [
        "### Data\n",
        "\n",
        "In this exercise we will use following synthetic data set and use  DEM, Slope,  TPI, MAT, MAP, NDVI, NLCD, FRG to fit Deep Neural Network regression model. This data was created with AI using gp_soil_data data set\n",
        "\n",
        "[gp_soil_data_syn.csv](https://www.dropbox.com/s/c63etg7u5qql2y8/gp_soil_data_syn.csv?dl=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "sw37LTo8Czaa",
        "outputId": "f1f06b37-0649-4eef-acbb-71db794ed28f"
      },
      "outputs": [
        {
          "ename": "ERROR",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "Error: package or namespace load failed for ‘tidyverse’ in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]):\n namespace ‘rlang’ 1.0.1 is already loaded, but >= 1.1.0 is required\nTraceback:\n",
            "1. library(tidyverse)",
            "2. tryCatch({\n .     attr(package, \"LibPath\") <- which.lib.loc\n .     ns <- loadNamespace(package, lib.loc)\n .     env <- attachNamespace(ns, pos = pos, deps, exclude, include.only)\n . }, error = function(e) {\n .     P <- if (!is.null(cc <- conditionCall(e))) \n .         paste(\" in\", deparse(cc)[1L])\n .     else \"\"\n .     msg <- gettextf(\"package or namespace load failed for %s%s:\\n %s\", \n .         sQuote(package), P, conditionMessage(e))\n .     if (logical.return && !quietly) \n .         message(paste(\"Error:\", msg), domain = NA)\n .     else stop(msg, call. = FALSE, domain = NA)\n . })",
            "3. tryCatchList(expr, classes, parentenv, handlers)",
            "4. tryCatchOne(expr, names, parentenv, handlers[[1L]])",
            "5. value[[3L]](cond)",
            "6. stop(msg, call. = FALSE, domain = NA)"
          ]
        }
      ],
      "source": [
        "library(tidyverse)\n",
        "# define file from my github\n",
        "urlfile = \"https://github.com//zia207/r-colab/raw/main/Data/USA/gp_soil_data_syn.csv\"\n",
        "mf<-read_csv(url(urlfile))\n",
        "# Create a data-frame\n",
        "df<-mf %>% dplyr::select(SOC, DEM, Slope, TPI,MAT, MAP,NDVI, NLCD, FRG)%>%\n",
        "    glimpse()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhfAkGx9DUCQ"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bo_aLdB3CtDF"
      },
      "source": [
        "### Convert to factor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mn9cgaf0EN4h"
      },
      "outputs": [],
      "source": [
        "df$NLCD <- as.factor(df$NLCD)\n",
        "df$FRG <- as.factor(df$FRG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZ0qCZ3WEOEd"
      },
      "source": [
        "### Data split\n",
        "\n",
        "The data set (n = 1408) will randomly split into sub-sets for training (70%), validation (15%) and test data (15%). The validation data will be used to optimized the model parameters during the tuning and training processes. The test data set will be used as the hold-out data to evaluate the DNN model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ve_p-bGAEYr9"
      },
      "outputs": [],
      "source": [
        "library(tidymodels)\n",
        "set.seed(1245)   # for reproducibility\n",
        "split_01 <- initial_split(df, prop = 0.7, strata = SOC)\n",
        "train <- split_01 %>% training()\n",
        "test_valid <-  split_01 %>% testing()\n",
        "\n",
        "split_02 <- initial_split(test_valid, prop = 0.5, strata = SOC)\n",
        "test <- split_02 %>% training()\n",
        "valid <-  split_02 %>% testing()\n",
        "\n",
        "# Density plot all, train and test data\n",
        "ggplot()+\n",
        "  geom_density(data = df, aes(SOC))+\n",
        "  geom_density(data = train, aes(SOC), color = \"green\")+\n",
        "  geom_density(data = test, aes(SOC), color = \"red\") +\n",
        "  geom_density(data = valid, aes(SOC), color = \"blue\") +\n",
        "      xlab(\"Soil Organic Carbon (kg/g)\") +\n",
        "     ylab(\"Density\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSFPg73-4r01"
      },
      "source": [
        "### Import h2o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gza80m674r_r"
      },
      "outputs": [],
      "source": [
        "library(h2o)\n",
        "h2o.init()\n",
        "#disable progress bar for RMarkdown\n",
        "h2o.no_progress()\n",
        "# Optional: remove anything from previous session\n",
        "h2o.removeAll()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6oXqUd741Xo"
      },
      "source": [
        "### Import data to h2o cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eEzVUGa41fI"
      },
      "outputs": [],
      "source": [
        "h_df=as.h2o(df)\n",
        "h_train = as.h2o(train)\n",
        "h_test = as.h2o(test)\n",
        "h_valid = as.h2o(valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQ8Y8RCt5BMy"
      },
      "outputs": [],
      "source": [
        "CV.xy<- as.data.frame(h_train)\n",
        "test.xy<- as.data.frame(h_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSLrJ05c5DHo"
      },
      "source": [
        "### Define response and predictors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVopw-Ah5DQ-"
      },
      "outputs": [],
      "source": [
        "y <- \"SOC\"\n",
        "x <- setdiff(names(h_df), y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1khSJw_5R-q"
      },
      "source": [
        "### Fit h2o model with few prameters\n",
        "\n",
        "First we fit DNN model with following parameters:\n",
        "\n",
        "standardize: logical. If enabled, automatically standardize the data.\n",
        "\n",
        "distribution:\n",
        "\n",
        "activation: Specify the activation function. One of:\n",
        "\n",
        "  - tanh\n",
        "  \n",
        "  - tanh_with_dropout\n",
        "  \n",
        "  - rectifier (default)\n",
        "  \n",
        "  - rectifier_with_dropout\n",
        "\n",
        "  - maxout (not supported when autoencoder is enabled)\n",
        "  \n",
        "  - maxout_with_dropout\n",
        "  \n",
        "hidden: Specify the hidden layer sizes (e.g., (100,100)). The value must be positive. This option defaults to (200,200).\n",
        "\n",
        "adaptive_rate: Specify whether to enable the adaptive learning rate (ADADELTA). This option defaults to True (enabled).\n",
        "\n",
        "epochs: Specify the number of times to iterate (stream) the dataset. The value can be a fraction. This option defaults to 10.\n",
        "\n",
        "epsilon: (Applicable only if adaptive_rate=True) Specify the adaptive learning rate time smoothing factor to avoid dividing by zero. This option defaults to 1e-08.\n",
        "\n",
        "input_dropout_ratio: Specify the input layer dropout ratio to improve generalization. Suggested values are 0.1 or 0.2. This option defaults to 0.\n",
        "\n",
        "l1: Specify the L1 regularization to add stability and improve generalization; sets the value of many weights to 0 (default).\n",
        "\n",
        "l2: Specify the L2 regularization to add stability and improve generalization; sets the value of many weights to smaller values. Defaults to 0.\n",
        "\n",
        "max_w2: Specify the constraint for the squared sum of the incoming weights per unit (e.g. for rectifier). Defaults to 3.4028235e+38.\n",
        "\n",
        "momentum_start: (Applicable only if adaptive_rate=False) Specify the initial momentum at the beginning of training; we suggest 0.5. This option defaults to 0.\n",
        "\n",
        "rate: (Applicable only if adaptive_rate=False) Specify the learning rate. Higher values result in a less stable model, while lower values lead to slower convergence. This option defaults to 0.005.\n",
        "\n",
        "rate_annealing: Learning rate decay, (Applicable only if adaptive_rate=False) Specify the rate annealing value. rate(1+ rate_annealing × samples), This option defaults to 1e-06.\n",
        "\n",
        "rate_decay: (Applicable only if adaptive_rate=False) Specify the rate decay factor between layers. N-th layer: rate × rate_decay(n−1). This options defaults to 1.\n",
        "\n",
        "regression_stop: (Regression models only) Specify the stopping criterion for regression error (MSE) on the training data. When the error is at or below this threshold, training stops. To disable this option, enter -1. This option defaults to 1e-06.\n",
        "\n",
        "rho: (Applicable only if adaptive_rate is enabled) Specify the adaptive learning rate time decay factor. This option defaults to 0.99.\n",
        "\n",
        "shuffle_training_data: Specify whether to shuffle the training data. This option is recommended if the training data is replicated and the value of train_samples_per_iteration is close to the number of nodes times the number of rows. This option defaults to False (disabled).\n",
        "\n",
        "stopping_tolerance = Relative tolerance for metric-based stopping criterion\n",
        "\n",
        "stopping_rounds = Early stopping based on convergence of stopping_metric.Defaults to 5.\n",
        "\n",
        "stopping_metric = Metric to use for early stopping\n",
        "\n",
        "variable_importances: Specify whether to compute variable importance. This option defaults to True (enabled)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9p4GXqu5SGI"
      },
      "outputs": [],
      "source": [
        "DNN <- h2o.deeplearning(\n",
        "                       model_id=\"DNN_model_ID\",\n",
        "                       training_frame=h_train,\n",
        "                       validation_frame=h_valid,\n",
        "                       x=x,\n",
        "                       y=y,\n",
        "                       distribution =\"AUTO\",\n",
        "                       standardize = TRUE,\n",
        "                       shuffle_training_data = TRUE,\n",
        "                       activation = \"tanh\",\n",
        "                       hidden = c(100, 100, 100),\n",
        "                       epochs = 500,\n",
        "                       adaptive_rate = TRUE,\n",
        "                       rate = 0.005,\n",
        "                       rate_annealing = 1e-06,\n",
        "                       rate_decay = 1,\n",
        "                       rho = 0.99,\n",
        "                       epsilon = 1e-08,\n",
        "                       momentum_start = 0.5,\n",
        "                       momentum_stable =0.99,\n",
        "                       input_dropout_ratio = 0.0001,\n",
        "                       regression_stop = 1e-06,\n",
        "                       l1 = 0.0001,\n",
        "                       l2 = 0.0001,\n",
        "                       max_w2 = 3.4028235e+38,\n",
        "                       stopping_tolerance = 0.001,\n",
        "                       stopping_rounds = 3,\n",
        "                       stopping_metric = \"RMSE\",\n",
        "                       nfolds = 5,\n",
        "                       keep_cross_validation_models = TRUE,\n",
        "                       keep_cross_validation_predictions = TRUE,\n",
        "                       variable_importances = TRUE,\n",
        "                       seed=1256\n",
        "                       )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EuFJGnO5c55"
      },
      "source": [
        "### Scoring history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Psl3qhJs5dCg"
      },
      "outputs": [],
      "source": [
        "plot(DNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOfTaSkM5qRo"
      },
      "source": [
        "### Model Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLZ4b7tl5rij"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJIARPr052D2"
      },
      "outputs": [],
      "source": [
        "h2o.performance(DNN,  h_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c64-lKP157LX"
      },
      "source": [
        "#### Cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBnruzKt57Ut"
      },
      "outputs": [],
      "source": [
        "h2o.performance(DNN,  xval=TRUE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deSWGO2G6IVK"
      },
      "source": [
        "#### Validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t63HUv9u6P6D"
      },
      "outputs": [],
      "source": [
        "h2o.performance(DNN,  h_valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeEaslPh6jum"
      },
      "source": [
        "#### Test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzTiUj-r7Eug"
      },
      "outputs": [],
      "source": [
        "h2o.performance(DNN, h_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFPEMoB07W72"
      },
      "source": [
        "### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ocrptlg6j1v"
      },
      "outputs": [],
      "source": [
        "test.pred.DNN<-as.data.frame(h2o.predict(object = DNN, newdata = h_test))\n",
        "test.xy$DNN_SOC<-test.pred.DNN$predict"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPm/AkIOV+B4SXM/gn5M+CT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "R",
      "name": "ir"
    },
    "language_info": {
      "name": "R"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}